# Structured JSON Logging

This document describes the structured JSON logging system implemented in the digital-actors project.

## Overview

The project uses `python-json-logger` to provide structured JSON logging with contextual data. Logs can be output in either JSON format (for production) or readable format (for development).

## Features

- **JSON Format**: Structured logs in JSON format for easy parsing by log aggregation tools (Datadog, Splunk, ELK stack, etc.)
- **Contextual Data**: Automatically includes session_id, character, scene, and other context in every log entry
- **Event Types**: Structured event logging with event_type field for filtering and analysis
- **Response Time Tracking**: Built-in timing information for performance monitoring
- **Environment-Based Configuration**: Automatically switches between JSON and readable formats based on environment
- **Field Renaming**: Maps timestamp → @timestamp and level → severity for common log aggregation tools

## Configuration

### Environment Variables

Control logging behavior with environment variables:

```bash
# Production (JSON format)
export ENV=production
export LOG_FORMAT_JSON=true

# Development (readable format)
export ENV=development
```

### Constants

Logging configuration is defined in `constants.py`:

```python
LOG_LEVEL_PRODUCTION: Final[str] = "INFO"
LOG_LEVEL_DEVELOPMENT: Final[str] = "DEBUG"
LOG_FORMAT_JSON: Final[bool] = True
```

## Usage

### Basic Setup

The logging system is automatically configured in `web_server.py`:

```python
from logging_config import setup_logging, StructuredLoggerAdapter
import logging

# Configure logging at application startup
setup_logging()  # Uses environment variables or defaults

# Get a logger
logger = logging.getLogger(__name__)
```

### Structured Logger Adapter

Use `StructuredLoggerAdapter` to add contextual information to all logs:

```python
from logging_config import StructuredLoggerAdapter
import logging

# Create a logger with session context
logger = logging.getLogger(__name__)
session_logger = StructuredLoggerAdapter(logger, {
    'session_id': session.session_id,
    'character': session.character_id,
    'scene': session.scene_id
})

# All logs from this adapter will include the context
session_logger.info("Message sent")  # Includes session_id, character, scene
```

### Event Logging

Log structured events with the event logging methods:

```python
# Info event with timing data
session_logger.info_event(
    "dialogue_generated",
    "Generated dialogue response",
    message_preview=message[:50],
    response_preview=response[:50],
    llm_response_time_ms=450,
    total_response_time_ms=520,
    sequence_id=1
)

# Error event with exception details
session_logger.error_event(
    "llm_api_error",
    "LLM API call failed",
    error=str(e),
    error_type=type(e).__name__,
    retry_count=3
)

# Warning event
session_logger.warning_event(
    "tts_failed",
    "TTS generation failed",
    error=str(e)
)

# Debug event
session_logger.debug_event(
    "response_queued",
    "Queued response",
    priority="NORMAL",
    content_preview=content[:50]
)
```

## Log Structure

### JSON Format (Production)

```json
{
  "@timestamp": "2026-01-30 21:09:59,262",
  "severity": "INFO",
  "logger": "web_server",
  "message": "Generated dialogue response",
  "event_type": "dialogue_generated",
  "session_id": "demo_session_123",
  "character": "clippy",
  "scene": "welcome",
  "message_preview": "Hello, how can I help?",
  "response_preview": "I can assist you with...",
  "llm_response_time_ms": 450,
  "total_response_time_ms": 520,
  "sequence_id": 1
}
```

### Readable Format (Development)

```
2026-01-30 21:09:59 - web_server - INFO - Generated dialogue response
```

## Common Event Types

The system logs the following event types:

### Session Management
- `session_created`: New session initialized
- `session_registered`: Session registered for authentication
- `session_token_sent`: Session token sent to client
- `session_acknowledged`: Client acknowledged session token
- `session_removed`: Session removed from active sessions

### WebSocket Events
- `websocket_connected`: Client connected
- `websocket_disconnected`: Client disconnected
- `websocket_error`: WebSocket error occurred

### Dialogue Generation
- `dialogue_generated`: Dialogue response generated by LLM
- `text_response_sent`: Text portion of response sent to client
- `audio_response_sent`: Audio (TTS) portion sent to client
- `response_queued`: Response added to queue
- `response_cancelled`: Response cancelled (superseded)

### System Events
- `player_memory_loaded`: Player memory system initialized
- `world_director_initialized`: World Director initialized
- `query_system_initialized`: Query System initialized
- `rag_facts_indexed`: RAG facts indexed for scene
- `options_sent`: Available characters/scenes sent to client

### Error Events
- `auth_failed`: Authentication failed
- `session_mismatch`: Session ID mismatch
- `json_decode_error`: Invalid JSON received
- `message_handler_error`: Error handling message
- `llm_api_error`: LLM API call failed
- `tts_failed`: TTS generation failed
- `tts_no_audio`: TTS returned no audio
- `suggestions_error`: Failed to generate suggestions

## Tracked Metrics

The logging system automatically tracks:

- **response_time_ms**: Total response time from request to completion
- **llm_response_time_ms**: Time spent in LLM API call
- **tts_time_ms**: Time spent generating TTS audio
- **sequence_id**: Sequence ID for tracking cancellations
- **content_preview**: First 50 characters of content (for debugging)

## Testing

Run the logging tests to verify the implementation:

```bash
python3 -m pytest tests/test_logging.py -v
```

Run the example script to see logging in action:

```bash
python3 example_logging.py
```

## Integration with Log Aggregation Tools

The JSON format is designed to work with common log aggregation platforms:

### Datadog

Logs are automatically parsed. No additional configuration needed.

### Splunk

Configure sourcetype as `_json` for automatic field extraction.

### ELK Stack (Elasticsearch, Logstash, Kibana)

The `@timestamp` field is recognized by Elasticsearch. Use Logstash with JSON codec:

```ruby
input {
  file {
    path => "/var/log/digital-actors/*.log"
    codec => "json"
  }
}
```

### CloudWatch Logs

Use JSON format with structured logging. Query using CloudWatch Insights:

```sql
fields @timestamp, severity, event_type, session_id, response_time_ms
| filter event_type = "dialogue_generated"
| stats avg(response_time_ms) by bin(5m)
```

## Best Practices

1. **Always use event types**: Use the `*_event()` methods instead of plain `info()`, `error()`, etc.
2. **Include context**: Add relevant contextual data as keyword arguments
3. **Preview long strings**: Use `[:50]` to truncate long content in logs
4. **Track timing**: Include `*_time_ms` fields for performance metrics
5. **Use structured logger**: Create a `StructuredLoggerAdapter` with session context
6. **Avoid PII**: Don't log sensitive user data (passwords, tokens, etc.)
7. **Use appropriate levels**: DEBUG for development, INFO for normal events, ERROR for failures

## Migration from Old Logging

The old logging style:

```python
logger.info("[AUTH] Generated session token: %s...", self.session_id[:8])
```

New structured logging style:

```python
self.logger.info_event("session_created", "Generated session token",
                        session_token_preview=self.session_id[:8])
```

## Environment Setup

For production deployment:

```bash
# .env file
ENV=production
LOG_FORMAT_JSON=true
LOG_LEVEL=INFO
```

For development:

```bash
# .env file
ENV=development
LOG_FORMAT_JSON=false
LOG_LEVEL=DEBUG
```
